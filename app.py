import streamlit as st
import urllib.request
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from datetime import datetime
from sklearn.calibration import calibration_curve
#import xmltodict
#from mitosheet.streamlit.v1 import spreadsheet
from pandas import json_normalize
#from streamlit_extras.add_vertical_space import add_vertical_space
from streamlit_lottie import st_lottie
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
# Set plot style
sns.set(color_codes=True)

# Page d'accueil
st.set_page_config(
    page_title="costumer churn prediction",
    page_icon="üåâ",
    layout="wide"
)

# D√©finissez la couleur de fond
st.markdown(
    """
    <style>
        body {
            background-color: #f4f4f4;
        }
    </style>
    """,
    unsafe_allow_html=True
)

def load_lottieurl(url: str):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

lottie_book = load_lottieurl("https://assets4.lottiefiles.com/temp/lf20_aKAfIn.json")
st_lottie(lottie_book, speed=1, height=200, key="initial")

# Ajouter un titre
st.title("Projet de Machine Learning pour Pr√©dire le Churn chez PowerCo : Vers une R√©tention Client Plus Efficace")
st.markdown(""" Ce projet s'inscrit dans la volont√© de PowerCo d'optimiser sa strat√©gie de r√©tention des clients PME 
et de renforcer sa position sur le march√© de l'√©nergie.
 
Je vous invite √† me suivre dans ma d√©marche de r√©solution de cette probl√©matique.""")
st.markdown("<p style='text-align: right;'> R√©alis√© par Blandine JATSA NGUETSE </p>", unsafe_allow_html=True)


# definition des fonctions utiles
def annotate_stacked_bars(ax, pad=0.99, colour="white", textsize=8):
    """
    Add value annotations to the bars
    """
    annotations = []

    # Iterate over the plotted rectangles/bars
    for p in ax.patches:
        # Calculate annotation
        value = str(round(p.get_height(), 1))
        # If value is 0 do not annotate
        if value == '0.0':
            continue
        annotation = ax.annotate(
            value,
            ((p.get_x() + p.get_width() / 2) * pad - 0.05, (p.get_y() + p.get_height() / 2) * pad),
            color=colour,
            size=textsize
        )
        annotations.append(annotation)

    return annotations

def plot_stacked_bars(dataframe, title_, size_=(18, 10), rot_=0, legend_="upper right"):
    """
    Plot stacked bars with annotations
    """
    fig, ax = plt.subplots(figsize=size_)

    dataframe.plot(
        kind="bar",
        stacked=True,
        ax=ax,
        rot=rot_,
        title=title_
    )

    # Annotate bars
    annotations = annotate_stacked_bars(ax, textsize=18)
    # Rename legend
    ax.legend(["Retention", "Churn"], loc=legend_)
    # Labels
    ax.set_ylabel("Base de clients (%)")

    # Display the Matplotlib figure in Streamlit
    st.pyplot(fig)

def display_data_types_info(dataframe):
    # Obtenir les types de donn√©es pour chaque variable
    data_types_info = dataframe.dtypes.reset_index()
    data_types_info.columns = ['Variable', 'Type']

    # Afficher les informations dans Streamlit
    #st.write("#### Informations sur les types de donn√©es:")
    st.table(data_types_info)

def plot_distribution(dataframe, column, bins_=50):
    """
    Plot variable distirbution in a stacked histogram of churned or retained company
    """
    # Create a temporal dataframe with the data to be plot
    temp = pd.DataFrame({"Retention": dataframe[dataframe["churn"]==0][column],
                         "Churn":dataframe[dataframe["churn"]==1][column]})

    # Plot the histogram
    fig, ax = plt.subplots(figsize=(8, 10))
    temp[["Retention","Churn"]].plot(kind='hist', bins=bins_, ax=ax, stacked=True)
    # X-axis label
    ax.set_xlabel(column)
    # Change the x-axis to plain style
    ax.ticklabel_format(style='plain', axis='x')
    st.pyplot(fig)

# importation des donn√©es
client_df = pd.read_csv('./client_data.csv')
price_df = pd.read_csv('./price_data.csv')

#churn
churn = client_df[['id', 'churn']]
churn.columns = ['Companies', 'churn']
churn_total = churn.groupby(churn['churn']).count()
churn_percentage = churn_total / churn_total.sum() * 100

#Sales channel
channel = client_df[['id', 'channel_sales', 'churn']]
channel = channel.groupby([channel['channel_sales'], channel['churn']])['id'].count().unstack(level=1).fillna(0)
channel_churn = (channel.div(channel.sum(axis=1), axis=0) * 100).sort_values(by=[1], ascending=False)


forecast = client_df[
    ["id", "forecast_cons_12m",
    "forecast_cons_year","forecast_discount_energy","forecast_meter_rent_12m",
    "forecast_price_energy_off_peak","forecast_price_energy_peak",
    "forecast_price_pow_off_peak","churn"
    ]
]
contract_type = client_df[['id', 'has_gas', 'churn']]
margin = client_df[['id', 'margin_gross_pow_ele', 'margin_net_pow_ele', 'net_margin']]
power = client_df[['id', 'pow_max', 'churn']]

client_df["date_activ"] = pd.to_datetime(client_df["date_activ"], format='%Y-%m-%d')
client_df["date_end"] = pd.to_datetime(client_df["date_end"], format='%Y-%m-%d')
client_df["date_modif_prod"] = pd.to_datetime(client_df["date_modif_prod"], format='%Y-%m-%d')
client_df["date_renewal"] = pd.to_datetime(client_df["date_renewal"], format='%Y-%m-%d')
price_df['price_date'] = pd.to_datetime(price_df['price_date'], format='%Y-%m-%d')


mean_year = price_df.groupby(['id']).mean().reset_index()
mean_6m = price_df[price_df['price_date'] > '2015-06-01'].groupby(['id']).mean().reset_index()
mean_3m = price_df[price_df['price_date'] > '2015-10-01'].groupby(['id']).mean().reset_index()


# Combinez en un seul dataframe
mean_year = mean_year.rename(
    index=str,
columns={
                    "price_off_peak_var": "mean_year_price_off_peak_var",
                    "price_peak_var": "mean_year_price_peak_var",
                    "price_mid_peak_var": "mean_year_price_mid_peak_var",
                    "price_off_peak_fix": "mean_year_price_off_peak_fix",
                    "price_peak_fix": "mean_year_price_peak_fix",
                    "price_mid_peak_fix": "mean_year_price_mid_peak_fix"
                }
            )

mean_year["mean_year_price_off_peak"] = mean_year["mean_year_price_off_peak_var"] + mean_year["mean_year_price_off_peak_fix"]
mean_year["mean_year_price_peak"] = mean_year["mean_year_price_peak_var"] + mean_year["mean_year_price_peak_fix"]
mean_year["mean_year_price_mid_peak"] = mean_year["mean_year_price_mid_peak_var"] + mean_year["mean_year_price_mid_peak_fix"]


mean_6m = mean_6m.rename(
                index=str,
                columns={
                    "price_off_peak_var": "mean_6m_price_off_peak_var",
                    "price_peak_var": "mean_6m_price_peak_var",
                    "price_mid_peak_var": "mean_6m_price_mid_peak_var",
                    "price_off_peak_fix": "mean_6m_price_off_peak_fix",
                    "price_peak_fix": "mean_6m_price_peak_fix",
                    "price_mid_peak_fix": "mean_6m_price_mid_peak_fix"
                }
            )

mean_6m["mean_6m_price_off_peak"] = mean_6m["mean_6m_price_off_peak_var"] + mean_6m["mean_6m_price_off_peak_fix"]
mean_6m["mean_6m_price_peak"] = mean_6m["mean_6m_price_peak_var"] + mean_6m["mean_6m_price_peak_fix"]
mean_6m["mean_6m_price_mid_peak"] = mean_6m["mean_6m_price_mid_peak_var"] + mean_6m["mean_6m_price_mid_peak_fix"]

mean_3m = mean_3m.rename(
    index=str,
    columns={
                    "price_off_peak_var": "mean_3m_price_off_peak_var",
                    "price_peak_var": "mean_3m_price_peak_var",
                    "price_mid_peak_var": "mean_3m_price_mid_peak_var",
                    "price_off_peak_fix": "mean_3m_price_off_peak_fix",
                    "price_peak_fix": "mean_3m_price_peak_fix",
                    "price_mid_peak_fix": "mean_3m_price_mid_peak_fix"
                }
            )

mean_3m["mean_3m_price_off_peak"] = mean_3m["mean_3m_price_off_peak_var"] + mean_3m["mean_3m_price_off_peak_fix"]
mean_3m["mean_3m_price_peak"] = mean_3m["mean_3m_price_peak_var"] + mean_3m["mean_3m_price_peak_fix"]
mean_3m["mean_3m_price_mid_peak"] = mean_3m["mean_3m_price_mid_peak_var"] + mean_3m["mean_3m_price_mid_peak_fix"]

# Merge into 1 dataframe
price_features = pd.merge(mean_year, mean_6m, on='id')
price_features = pd.merge(price_features, mean_3m, on='id')
price_analysis = pd.merge(price_features, client_df[['id', 'churn']], on='id')
# display_data_types_info(price_features)
# display_data_types_info(price_analysis)
# Calcul de la matrice de corr√©lation
price_analysis_copy = price_analysis.drop('id', axis=1)
merged_data = pd.merge(client_df.drop(columns=['churn']), price_analysis, on='id')
#merged_data.to_csv('clean_data_after_eda.csv')

#Feature Engineering
df = merged_data.copy()
df["date_activ"] = pd.to_datetime(df["date_activ"], format='%Y-%m-%d')
df["date_end"] = pd.to_datetime(df["date_end"], format='%Y-%m-%d')
df["date_modif_prod"] = pd.to_datetime(df["date_modif_prod"], format='%Y-%m-%d')
df["date_renewal"] = pd.to_datetime(df["date_renewal"], format='%Y-%m-%d')
price_df = pd.read_csv('price_data.csv')
price_df["price_date"] = pd.to_datetime(price_df["price_date"], format='%Y-%m-%d')
# Group off-peak prices by companies and month
monthly_price_by_id = price_df.groupby(['id', 'price_date']).agg(
    {'price_off_peak_var': 'mean', 'price_off_peak_fix': 'mean'}).reset_index()

# Get january and december prices
jan_prices = monthly_price_by_id.groupby('id').first().reset_index()
dec_prices = monthly_price_by_id.groupby('id').last().reset_index()

# Calculate the difference
diff = pd.merge(dec_prices.rename(columns={'price_off_peak_var': 'dec_1', 'price_off_peak_fix': 'dec_2'}),
                    jan_prices.drop(columns='price_date'), on='id')
diff['offpeak_diff_dec_january_energy'] = diff['dec_1'] - diff['price_off_peak_var']
diff['offpeak_diff_dec_january_power'] = diff['dec_2'] - diff['price_off_peak_fix']
diff = diff[['id', 'offpeak_diff_dec_january_energy', 'offpeak_diff_dec_january_power']]
df = pd.merge(df, diff, on='id')

# Aggregate average prices per period by company
mean_prices = price_df.groupby(['id']).agg({
        'price_off_peak_var': 'mean',
        'price_peak_var': 'mean',
        'price_mid_peak_var': 'mean',
        'price_off_peak_fix': 'mean',
        'price_peak_fix': 'mean',
        'price_mid_peak_fix': 'mean'
    }).reset_index()

# Calculate the mean difference between consecutive periods
mean_prices['off_peak_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_peak_var']
mean_prices['peak_mid_peak_var_mean_diff'] = mean_prices['price_peak_var'] - mean_prices['price_mid_peak_var']
mean_prices['off_peak_mid_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices[
        'price_mid_peak_var']
mean_prices['off_peak_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_peak_fix']
mean_prices['peak_mid_peak_fix_mean_diff'] = mean_prices['price_peak_fix'] - mean_prices['price_mid_peak_fix']
mean_prices['off_peak_mid_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices[
        'price_mid_peak_fix']
columns = [
        'id',
        'off_peak_peak_var_mean_diff',
        'peak_mid_peak_var_mean_diff',
        'off_peak_mid_peak_var_mean_diff',
        'off_peak_peak_fix_mean_diff',
        'peak_mid_peak_fix_mean_diff',
        'off_peak_mid_peak_fix_mean_diff'
    ]
df = pd.merge(df, mean_prices[columns], on='id')

# Aggregate average prices per period by company
mean_prices_by_month = price_df.groupby(['id', 'price_date']).agg({
        'price_off_peak_var': 'mean',
        'price_peak_var': 'mean',
        'price_mid_peak_var': 'mean',
        'price_off_peak_fix': 'mean',
        'price_peak_fix': 'mean',
        'price_mid_peak_fix': 'mean'
    }).reset_index()

# Calculate the mean difference between consecutive periods
mean_prices_by_month['off_peak_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - \
                                                          mean_prices_by_month['price_peak_var']
mean_prices_by_month['peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_peak_var'] - mean_prices_by_month[
        'price_mid_peak_var']
mean_prices_by_month['off_peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - \
                                                              mean_prices_by_month['price_mid_peak_var']
mean_prices_by_month['off_peak_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - \
                                                          mean_prices_by_month['price_peak_fix']
mean_prices_by_month['peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_peak_fix'] - mean_prices_by_month[
        'price_mid_peak_fix']
mean_prices_by_month['off_peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - \
                                                              mean_prices_by_month['price_mid_peak_fix']
# Calculate the maximum monthly difference across time periods
max_diff_across_periods_months = mean_prices_by_month.groupby(['id']).agg({
        'off_peak_peak_var_mean_diff': 'max',
        'peak_mid_peak_var_mean_diff': 'max',
        'off_peak_mid_peak_var_mean_diff': 'max',
        'off_peak_peak_fix_mean_diff': 'max',
        'peak_mid_peak_fix_mean_diff': 'max',
        'off_peak_mid_peak_fix_mean_diff': 'max'
    }).reset_index().rename(
    columns={
            'off_peak_peak_var_mean_diff': 'off_peak_peak_var_max_monthly_diff',
            'peak_mid_peak_var_mean_diff': 'peak_mid_peak_var_max_monthly_diff',
            'off_peak_mid_peak_var_mean_diff': 'off_peak_mid_peak_var_max_monthly_diff',
            'off_peak_peak_fix_mean_diff': 'off_peak_peak_fix_max_monthly_diff',
            'peak_mid_peak_fix_mean_diff': 'peak_mid_peak_fix_max_monthly_diff',
            'off_peak_mid_peak_fix_mean_diff': 'off_peak_mid_peak_fix_max_monthly_diff'
        }
    )
columns = [
        'id',
        'off_peak_peak_var_max_monthly_diff',
        'peak_mid_peak_var_max_monthly_diff',
        'off_peak_mid_peak_var_max_monthly_diff',
        'off_peak_peak_fix_max_monthly_diff',
        'peak_mid_peak_fix_max_monthly_diff',
        'off_peak_mid_peak_fix_max_monthly_diff'
    ]

df = pd.merge(df, max_diff_across_periods_months[columns], on='id')
#df['tenure'] = ((df['date_end'] - df['date_activ']) / np.timedelta64(1, 'Y')).astype(int)
df['tenure'] = ((df['date_end'] - df['date_activ']).dt.days / 365).astype(int)

def convert_months(reference_date, df, column):
    """
    Input a column with timedeltas and return months
    """
    time_delta = reference_date - df[column]
    months = (time_delta.dt.days / (365.25 / 12)).astype(int)
    return months

# Create reference date
reference_date = datetime(2016, 1, 1)

# Create columns
df['months_activ'] = convert_months(reference_date, df, 'date_activ')
df['months_to_end'] = -convert_months(reference_date, df, 'date_end')
df['months_modif_prod'] = convert_months(reference_date, df, 'date_modif_prod')
df['months_renewal'] = convert_months(reference_date, df, 'date_renewal')

df['has_gas'] = df['has_gas'].replace(['t', 'f'], [1, 0])
# Transform into categorical type
df['channel_sales'] = df['channel_sales'].astype('category')
df = pd.get_dummies(df, columns=['channel_sales'], prefix='channel')
df = df.drop(columns=['channel_sddiedcslfslkckwlfkdpoeeailfpeds', 'channel_epumfxlbckeskwekxbiuasklxalciiuu',
                          'channel_fixdbufsefwooaasfcxdxadsiekoceaa'])

# Transform into categorical type
df['origin_up'] = df['origin_up'].astype('category')
df = pd.get_dummies(df, columns=['origin_up'], prefix='origin_up')
df = df.drop(columns=['origin_up_MISSING', 'origin_up_usapbepcfoloekilkwsdiboslwaxobdp',
                          'origin_up_ewxeelcelemmiwuafmddpobolfuxioce'])
skewed = [
        'cons_12m',
        'cons_gas_12m',
        'cons_last_month',
        'forecast_cons_12m',
        'forecast_cons_year',
        'forecast_discount_energy',
        'forecast_meter_rent_12m',
        'forecast_price_energy_off_peak',
        'forecast_price_energy_peak',
        'forecast_price_pow_off_peak'
    ]
skewed_before = df[skewed].describe()

df["cons_12m"] = np.log10(df["cons_12m"] + 1)
df["cons_gas_12m"] = np.log10(df["cons_gas_12m"] + 1)
df["cons_last_month"] = np.log10(df["cons_last_month"] + 1)
df["forecast_cons_12m"] = np.log10(df["forecast_cons_12m"] + 1)
df["forecast_cons_year"] = np.log10(df["forecast_cons_year"] + 1)
df["forecast_meter_rent_12m"] = np.log10(df["forecast_meter_rent_12m"] + 1)
df["imp_cons"] = np.log10(df["imp_cons"] + 1)

skewed_after = df[skewed].describe()



df = df.drop(columns=['num_years_antig', 'forecast_cons_year'])
#df.to_csv("data_after_featuring.csv", index=False)

# Mod√©lisation
df_model = df.copy()

#df_copy = df_model.drop(['Unnamed: 0', 'id', 'price_date_x', 'price_date_y', 'price_date', 'date_activ',
#                         'date_end', 'date_modif_prod', 'date_renewal'], axis=1)
df_copy = df_model.drop(['id', 'price_date_x', 'price_date_y', 'price_date', 'date_activ',
                         'date_end', 'date_modif_prod', 'date_renewal'], axis=1)
train_df = df_copy.copy()

# Separate target variable from independent variables
y = df_model['churn']
#X = df_model.drop(columns=['id', 'churn', 'Unnamed: 0', 'price_date_x', 'price_date_y', 'price_date', 'date_activ',
#                          'date_end', 'date_modif_prod', 'date_renewal' ])
X = df_model.drop(columns=['id', 'churn', 'price_date_x', 'price_date_y', 'price_date', 'date_activ',
                           'date_end', 'date_modif_prod', 'date_renewal' ])
#print(X.shape)
#print(y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
model = RandomForestClassifier(
        n_estimators=1000
    )
model.fit(X_train, y_train)
predictions = model.predict(X_test)
tn, fp, fn, tp = metrics.confusion_matrix(y_test, predictions).ravel()
#st.dataframe(y_test.value_counts())
feature_importances = pd.DataFrame({
        'features': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values(by='importance', ascending=True).reset_index()

proba_predictions = model.predict_proba(X_test)
probabilities = proba_predictions[:, 1]
X_test = X_test.reset_index()
X_test.drop(columns='index', inplace=True)
X_test['churn'] = predictions.tolist()
X_test['churn_probability'] = probabilities.tolist()

#X_test.to_csv('./out_of_sample_data_with_predictions.csv')

# impact business de la remise

test_df = X_test.copy()
# Le chiffre d'affaires d'√©lectricit√© pour chaque client est compos√© de la consommation d'√©nergie (quantit√© * prix)
# et du loyer du compteur
# (Le prix de l'√©lectricit√© peut √©galement jouer un r√¥le, mais nous allons l'ignorer pour l'instant car nous avons
# besoin de demander au client davantage de donn√©es.)
# Notons que nous devons inverser la transformation log10 effectu√©e lors de l'√©tape de nettoyage des donn√©es.
test_df['basecase_revenue'] = ((np.power(10, test_df['forecast_cons_12m']) + 1) * test_df['forecast_price_energy_off_peak']
                               + test_df['forecast_meter_rent_12m'])

# En tenant compte du churn
test_df['basecase_revenue_after_churn'] = test_df['basecase_revenue'] * (1 - 0.919 * test_df['churn'])
df_revenu = test_df.head()

def get_rev_delta(pred: pd.DataFrame, cutoff: float=0.5, discount: float=0.2) -> float:
    """
    Get the delta of revenues for offering discount for all customers with predicted churn risk >= cutoff
    """
    pred['discount_revenue'] = pred['basecase_revenue_after_churn']
    # Churn predicted => discount is given => customer stays for full year, independent of whether the prediction
    # (false positive, "free"/unnecessary discount given) or correct
    pred.loc[pred['churn_probability'] >= cutoff, 'discount_revenue'] = pred['basecase_revenue'] * (1 - discount)
    # Save the revenue delta for each customer in a separate column
    pred['revenue_delta'] = pred['discount_revenue'] - pred['basecase_revenue_after_churn']
    return pred['revenue_delta'].sum()

# Generate a list of possible cutoffs and the corresponding overall revenue deltas
rev_deltas = pd.Series({cutoff: get_rev_delta(test_df, cutoff=cutoff) for cutoff in np.arange(0, 1, 0.01)})

def plot_tradeoff(rev_deltas: pd.Series):
    # Plot the revenue deltas
    fig, ax = plt.subplots()
    rev_deltas.plot(ax=ax)
    # Mark optimal point
    max_pred = rev_deltas.idxmax()
    ax.scatter(max_pred, rev_deltas.loc[max_pred], s=100, c='red')
    # Reference line for break-even
    ax.hlines(0, 0, 1)
    plt.show()
    print(f'Maximum benefit at cutoff {max_pred} with revenue delta of ${rev_deltas.loc[max_pred]:,.2f}')
    st.pyplot(fig)

# comment choisir le seuil de coupure ?

def get_rev_delta_high_value(pred: pd.DataFrame, cutoff: float=0.5, discount: float=0.2, min_rev: float=500):
    """
    Get the delta of revenues for offering discount for all customers with predicted churn risk >= cutoff and rev
    """
    pred['discount_revenue'] = pred['basecase_revenue_after_churn']
    # Churn predicted => discount is given for high-value customers => customer stays for full year, independent
    # (false positive, "free"/unnecessary discount given) or correct
    pred.loc[(pred['churn_probability'] >= cutoff) & (pred['basecase_revenue'] > min_rev),
    'discount_revenue'] = pred['basecase_revenue'] * (1 - discount)
    # Save the revenue delta for each customer in a separate column
    pred['revenue_delta'] = pred['discount_revenue'] - pred['basecase_revenue_after_churn']
    return pred['revenue_delta'].sum()

# G√©n√©rons une liste de seuils possibles et les deltas de revenus globaux correspondants
rev_deltas_high_value = pd.Series({cutoff: get_rev_delta_high_value(test_df, cutoff=cutoff) for cutoff in np.arange(0, 1, 0.01)})

# Les revenus d'√©lectricit√© pour chaque client comprennent la consommation d'√©nergie (quantit√© * prix) et la location
# du compteur.
# Le prix de l'√©lectricit√© peut √©galement jouer un r√¥le, mais nous allons l'ignorer pour l'instant car nous devons
# demander plus d'informations au client.
# Notons que nous devons inverser la transformation logarithmique en base 10 effectu√©e lors de l'√©tape de nettoyage
# des donn√©es.
test_df['basecase_revenue'] = (np.power(10, test_df['forecast_cons_12m']) * test_df['forecast_price_energy_off_peak']
                               + test_df['forecast_meter_rent_12m'])
# Taking churn into account
test_df['basecase_revenue_after_churn'] = test_df['basecase_revenue'] * (1 - 0.919 * test_df['churn_probability'])

# Generate a list of possible cutoffs and the corresponding overall revenue deltas
rev_deltas = pd.Series({cutoff: get_rev_delta(test_df, cutoff=cutoff) for cutoff in np.arange(0, 1, 0.01)})





st.sidebar.title("Menu de Navigation")
selected_option = st.sidebar.radio("Plan de travail", ["Comprehension du besoin client",
                                                       "Presentation du jeu de donn√©es", "Exploration des donn√©es",
                                                       "Exploration des Hypoth√®ses", "Feature Engineering",
                                                       "Mod√©lisation", "Impact Business de la Remise de 20%",
                                                       "Conclusion et Recommandations"])

if selected_option == "Comprehension du besoin client":
    st.subheader("Contexte ")
    st.markdown("""
    PowerCo est un important fournisseur de gaz et d'√©lectricit√© qui approvisionne des entreprises, des PME et des
     clients r√©sidentiels. La lib√©ralisation du march√© de l'√©nergie en Europe a entra√Æn√© un taux de d√©sabonnement 
     important, en particulier dans le segment des PME.
     
     Elle cherche √† comprendre les raisons derri√®re le taux significatif de d√©sabonnement dans le segment des PME.
     
     l'entreprise explore les hypoth√®ses suivantes :
     
     - Les variations de prix ont un impact direct sur la perte de clients
     - Pour les clients qui risquent de changer de fournisseur, une remise  de 20 % pourrait les inciter √† rester
      chez nous. """)
    st.subheader("M√©thodologie de r√©solution")
    st.markdown("""
    Afin de tester l'hypoth√®se selon laquelle le d√©sabonnement est li√© √† la sensibilit√© des clients aux prix, nous 
    devons mod√©liser les probabilit√©s de d√©sabonnement des clients et calculer l'effet des prix sur les taux de 
    d√©sabonnement. Nous aurons besoin des donn√©es suivantes pour pouvoir construire les mod√®les.
    
**Donn√©es n√©cessaires** :
- Donn√©es sur les clients - qui devraient inclure les caract√©ristiques de chaque client, par exemple,
le secteur d'activit√©, l'historique de la consommation d'√©lectricit√©, la date √† laquelle le client est devenu client,
 etc.
- Donn√©es sur le d√©sabonnement - qui doivent indiquer si le client s'est d√©sabonn√©.
- Donn√©es historiques sur les prix - qui doivent indiquer les prix factur√©s par le client √† chaque client,  tant pour 
l'√©lectricit√© que pour le gaz, au moment de l'achat.

Une fois que nous aurons les donn√©es, le plan de travail sera le suivant
1. Nous devrions d√©finir ce qu'est la sensibilit√© aux prix et la calculer.
2. Nous devrions concevoir des caract√©ristiques bas√©es sur les donn√©es que nous obtenons, et construire un mod√®le de 
classification binaire (par exemple, r√©gression logistique, Random Forest, Gradient Boosting).
3. Le meilleur mod√®le sera choisi en fonction du compromis entre la complexit√©, l'explicabilit√© et la pr√©cision
 des mod√®les.
4. Nous approfondirons ensuite la question de savoir pourquoi et comment les changements de prix ont un impact sur 
le taux de d√©sabonnement.
5. Enfin, le mod√®le nous permettrait d'√©valuer l'impact commercial de la strat√©gie d'actualisation propos√©e par 
le client.
    """)
    st.markdown(""" On y va !!! """)

if selected_option == "Presentation du jeu de donn√©es":
    st.markdown(""" Nous disposons d'un ensemble de donn√©es comprenant les caract√©ristiques des PME clientes en janvier
     2016 ainsi que des informations indiquant si elles se sont d√©sabonn√©es ou non en mars 2016. 
     En outre, nous avons re√ßu les prix de 2015 pour ces clients.""")

    afficher_paragraphe2 = st.checkbox("Jeu de donn√©es")
    if afficher_paragraphe2:
        st.dataframe(client_df.head(3))
        st.markdown(""" Les donn√©es relatives aux clients sont un m√©lange de donn√©es num√©riques et cat√©gorielles, 
        que nous devrons transformer avant de les mod√©liser ult√©rieurement """)
        st.dataframe(price_df.head(3))
        st.markdown("""En ce qui concerne les donn√©es relatives aux prix, il s'agit de donn√©es essentiellement
         num√©riques, mais nous pouvons constater la pr√©sence de valeurs nulles """)

    stat_desc = st.checkbox("Statistiques descriptives")
    if stat_desc:
        st.write("#### Informations sur les types de donn√©es:")
        display_data_types_info(client_df)
        display_data_types_info(price_df)
        st.write("Concernant les donn√©es client, nous constatons que toutes les variables li√©es √† la date ne sont pas actuellement au format datetime."
                 " Nous devrons les convertir ult√©rieurement.")
        st.write("#### Statistics")
        st.dataframe(client_df.describe())
        st.markdown(""" Le point essentiel √† retenir est que nous avons des donn√©es fortement asym√©triques, comme le 
        montrent les valeurs des percentiles.""")
        st.dataframe(price_df.describe())
        st.markdown("Dans l'ensemble, les donn√©es relatives aux prix sont satisfaisantes.")

if selected_option == "Exploration des donn√©es" :
    st.write("Voyons maintenant un peu plus en d√©tail les donn√©es clients et prix.")
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Taux d'attrition")
        plot_stacked_bars(churn_percentage.transpose(), "taux d'attrition", (5, 5), legend_="lower right")
    with col2:
        st.subheader("Canal de vente")
        plot_stacked_bars(channel_churn, 'Canal de vente', rot_=30)
        st.write(""" Les clients qui r√©silient sont r√©partis sur 5 valeurs diff√©rentes pour le canal de vente. De plus,
         la valeur "MISSING" a un taux d'attrition de 7,6 %. "MISSING" indique une valeur manquante. Cette 
         caract√©ristique pourrait √™tre importante lors de la construction de notre mod√®le.""")

    st.subheader("Consommation")
    st.write("""
Voyons la distribution de la consommation au cours de la derni√®re ann√©e et du dernier mois. Comme les donn√©es de
 consommation sont univari√©es, utilisons des histogrammes pour visualiser leur distribution.""")
    consumption = client_df[['id', 'cons_12m', 'cons_gas_12m', 'cons_last_month', 'imp_cons', 'has_gas', 'churn']]
    variable_list = ['cons_12m', 'cons_gas_12m', 'cons_last_month', 'imp_cons']
    selected_variable = st.selectbox("S√©lectionnez la variable √† afficher :", variable_list)
    if selected_variable == "cons_gas_12m":
        plot_distribution(consumption[consumption['has_gas'] == 't'], selected_variable)

    else:
        plot_distribution(consumption, selected_variable)

    st.write(""" les donn√©es de consommation pr√©sentent une forte asym√©trie positive, avec une tr√®s longue queue droite
     vers les valeurs plus √©lev√©es de la distribution. Les valeurs √† l'extr√©mit√© sup√©rieure et inf√©rieure de la 
     distribution sont susceptibles d'√™tre des valeurs aberrantes. 
     
     Nous pouvons utiliser un graphique standard pour visualiser les valeurs aberrantes plus en d√©tail. 
     Une bo√Æte √† moustaches (boxplot) est une m√©thode standardis√©e d'affichage de la distribution bas√©e sur un
      r√©sum√© en cinq nombres :
      - Minimum
      - Premier quartile (Q1)
      - M√©diane
      - Troisi√®me quartile (Q3)
      - Maximum
      
      Il peut r√©v√©ler les outliers et quelles sont leurs valeurs. Il peut √©galement nous indiquer si nos donn√©es 
      sont sym√©triques, √† quel point nos donn√©es sont regroup√©es et si/comment nos donn√©es sont asym√©triques.
        """)
    fig, axs = plt.subplots(nrows=4, figsize=(8, 10))

    # Plot histogram
    sns.boxplot(consumption["cons_12m"], orient="h", ax=axs[0])
    sns.boxplot(consumption[consumption["has_gas"] == "t"]["cons_gas_12m"], orient="h", ax=axs[1])
    sns.boxplot(consumption["cons_last_month"], orient="h", ax=axs[2])
    sns.boxplot(consumption["imp_cons"], orient="h", ax=axs[3])

    axs[0].set_xlim(-200000, 2000000)
    axs[1].set_xlim(-200000, 2000000)
    axs[2].set_xlim(-20000, 100000)
    st.pyplot(fig)
    st.subheader("Pr√©vision")
    variable_forecast = ["forecast_cons_12m", "forecast_cons_year", "forecast_discount_energy",
                         "forecast_meter_rent_12m",
                         "forecast_price_energy_off_peak", "forecast_price_energy_peak", "forecast_price_pow_off_peak"]
    selected_var_forecast = st.selectbox("S√©lectionnez la variable √† afficher :", variable_forecast)
    plot_distribution(client_df, selected_var_forecast)
    st.write(""" De mani√®re similaire aux graphiques de consommation, nous pouvons observer que de nombreuses variables
     pr√©sentent une forte asym√©trie positive, cr√©ant une queue tr√®s longue pour les valeurs plus √©lev√©es. 
     Nous comptons effectuer certaines transformations pour corriger cette asym√©trie.""")

    st.subheader("Type de contrat")
    contract = contract_type.groupby([contract_type['churn'], contract_type['has_gas']])['id'].count().unstack(level=0)
    contract_percentage = (contract.div(contract.sum(axis=1), axis=0) * 100).sort_values(by=[1], ascending=False)
    plot_stacked_bars(contract_percentage, 'Type de contrat avec le gaz')

    st.subheader("Marges")
    fig, axs = plt.subplots(nrows=3, figsize=(18, 20))
    # Plot histogram
    sns.boxplot(margin["margin_gross_pow_ele"], orient="h", ax=axs[0])
    sns.boxplot(margin["margin_net_pow_ele"], orient="h", ax=axs[1])
    sns.boxplot(margin["net_margin"], orient="h", ax=axs[2])
    st.pyplot(fig)
    st.write("""Nous pouvons √©galement observer quelques valeurs aberrantes ici, que nous traiterons par la suite.""")
    st.subheader("L'√©nergie souscrite")
    plot_distribution(power, 'pow_max')

    st.subheader("Le reste des variables")
    others = client_df[['id', 'nb_prod_act', 'num_years_antig', 'origin_up', 'churn']]
    variable_list_others = ['nb_prod_act', 'num_years_antig', 'origin_up']
    selected_other_var = st.selectbox("S√©lectionnez la variable √† afficher :", variable_list_others)

    if selected_other_var == 'nb_prod_act':
        products = others.groupby([others[selected_other_var], others["churn"]])["id"].count().unstack(level=1)
        products_percentage = (products.div(products.sum(axis=1), axis=0) * 100).sort_values(by=[1], ascending=False)
        plot_stacked_bars(products_percentage, "Number of products")
    elif selected_other_var == 'num_years_antig':
        years_antig = others.groupby([others[selected_other_var], others["churn"]])["id"].count().unstack(level=1)
        years_antig_percentage = (years_antig.div(years_antig.sum(axis=1), axis=0) * 100)
        plot_stacked_bars(years_antig_percentage, "Number years")
    else:
        origin = others.groupby([others[selected_other_var], others["churn"]])["id"].count().unstack(level=1)
        origin_percentage = (origin.div(origin.sum(axis=1), axis=0) * 100)
        plot_stacked_bars(origin_percentage, "Origin contract/offer")


if selected_option == "Exploration des Hypoth√®ses":
    st.write("""Maintenant que nous avons explor√© les donn√©es, il est temps d'investiguer si la sensibilit√© au prix a 
    une influence sur le d√©sabonnement. Tout d'abord, nous devons d√©finir pr√©cis√©ment ce qu'est la sensibilit√© au prix.
    
    √âtant donn√© que nous disposons des donn√©es de consommation pour chacune des entreprises pour l'ann√©e 2015, 
    nous allons cr√©er de nouvelles caract√©ristiques pour mesurer la "sensibilit√© au prix" en utilisant la moyenne
     de l'ann√©e, des 6 derniers mois et des 3 derniers mois.
        """)
    transform_price = st.checkbox("Caract√©ristiques de la sensibilit√© au prix (cochez cette petite case pour"
                                  " comprendre la creation des nouvelles variables associ√©es)")
    if transform_price:
        st.subheader("Transformer les colonnes de date en type datetime")
        code_datetime = """
            client_df["date_activ"] = pd.to_datetime(client_df["date_activ"], format='%Y-%m-%d')
            client_df["date_end"] = pd.to_datetime(client_df["date_end"], format='%Y-%m-%d')
            client_df["date_modif_prod"] = pd.to_datetime(client_df["date_modif_prod"], format='%Y-%m-%d')
            client_df["date_renewal"] = pd.to_datetime(client_df["date_renewal"], format='%Y-%m-%d')
            price_df['price_date'] = pd.to_datetime(price_df['price_date'], format='%Y-%m-%d')
            """
        st.code(code_datetime, language="python")

        st.subheader("Cr√©er des donn√©es de moyenne pond√©r√©e")
        code_mean = """
                mean_year = price_df.groupby(['id']).mean().reset_index()
                mean_6m = price_df[price_df['price_date'] > '2015-06-01'].groupby(['id']).mean().reset_index()
                mean_3m = price_df[price_df['price_date'] > '2015-10-01'].groupby(['id']).mean().reset_index()

                # Combiner en un seul dataframe
                mean_year = mean_year.rename(
                    index=str,
                    columns={
                        "price_off_peak_var": "mean_year_price_off_peak_var",
                        "price_peak_var": "mean_year_price_peak_var",
                        "price_mid_peak_var": "mean_year_price_mid_peak_var",
                        "price_off_peak_fix": "mean_year_price_off_peak_fix",
                        "price_peak_fix": "mean_year_price_peak_fix",
                        "price_mid_peak_fix": "mean_year_price_mid_peak_fix"
                    }
                )

                mean_year["mean_year_price_off_peak"] = mean_year["mean_year_price_off_peak_var"] + mean_year["mean_year_price_off_peak_fix"]
                mean_year["mean_year_price_peak"] = mean_year["mean_year_price_peak_var"] + mean_year["mean_year_price_peak_fix"]
                mean_year["mean_year_price_mid_peak"] = mean_year["mean_year_price_mid_peak_var"] + mean_year["mean_year_price_mid_peak_fix"]

                mean_6m = mean_6m.rename(
                    index=str,
                    columns={
                        "price_off_peak_var": "mean_6m_price_off_peak_var",
                        "price_peak_var": "mean_6m_price_peak_var",
                        "price_mid_peak_var": "mean_6m_price_mid_peak_var",
                        "price_off_peak_fix": "mean_6m_price_off_peak_fix",
                        "price_peak_fix": "mean_6m_price_peak_fix",
                        "price_mid_peak_fix": "mean_6m_price_mid_peak_fix"
                    }
                )
                mean_6m["mean_6m_price_off_peak"] = mean_6m["mean_6m_price_off_peak_var"] + mean_6m["mean_6m_price_off_peak_fix"]
                mean_6m["mean_6m_price_peak"] = mean_6m["mean_6m_price_peak_var"] + mean_6m["mean_6m_price_peak_fix"]
                mean_6m["mean_6m_price_mid_peak"] = mean_6m["mean_6m_price_mid_peak_var"] + mean_6m["mean_6m_price_mid_peak_fix"]

                mean_3m = mean_3m.rename(
                    index=str,
                    columns={
                        "price_off_peak_var": "mean_3m_price_off_peak_var",
                        "price_peak_var": "mean_3m_price_peak_var",
                        "price_mid_peak_var": "mean_3m_price_mid_peak_var",
                        "price_off_peak_fix": "mean_3m_price_off_peak_fix",
                        "price_peak_fix": "mean_3m_price_peak_fix",
                        "price_mid_peak_fix": "mean_3m_price_mid_peak_fix"
                    }
                )
                mean_3m["mean_3m_price_off_peak"] = mean_3m["mean_3m_price_off_peak_var"] + mean_3m["mean_3m_price_off_peak_fix"]
                mean_3m["mean_3m_price_peak"] = mean_3m["mean_3m_price_peak_var"] + mean_3m["mean_3m_price_peak_fix"]
                mean_3m["mean_3m_price_mid_peak"] = mean_3m["mean_3m_price_mid_peak_var"] + mean_3m["mean_3m_price_mid_peak_fix"]

                # Fusionner en seul dataframe
                price_features = pd.merge(mean_year, mean_6m, on='id')
                price_features = pd.merge(price_features, mean_3m, on='id')

                    """
        st.code(code_mean, language="python")

    st.dataframe(price_features.head())

    st.write("Maintenant, fusionnons les donn√©es de churn et voyons s'il y a une corr√©lation avec la sensibilit√© "
             "au prix.")

    corr = price_analysis_copy.corr()
    fig, ax = plt.subplots(figsize=(20, 18))
    sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True,
                annot_kws={'size': 10})

    # Ajustement de la taille des √©tiquettes d'axe
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)

    # Affichage du graphique dans Streamlit en passant explicitement la figure
    st.pyplot(fig)
    st.write(""" D'apr√®s le graphique de corr√©lation, il montre une plus grande intensit√© de corr√©lation entre les
     autres variables de sensibilit√© au prix, cependant, dans l'ensemble, la corr√©lation avec le d√©sabonnement est
      tr√®s faible. Cela indique qu'il existe une faible relation lin√©aire entre la sensibilit√© au prix et 
      le d√©sabonnement. Cela sugg√®re que, pour que la sensibilit√© au prix soit un facteur majeur dans la pr√©diction
       du taux d'attrition, nous devrons peut-√™tre modifier la transformation des caracteristiques correspondantes.""")

    st.dataframe(merged_data.head(3))
    num_rows, num_columns = merged_data.shape
    st.write(f"La base de donn√©es nettoy√©s a {num_rows} lignes et {num_columns} colonnes.")
    # st.text(merged_data.columns)

if selected_option == "Feature Engineering":
    st.subheader("Diff√©rence entre les prix hors pointe en d√©cembre et janvier pr√©c√©dent")
    st.write("""Ci-dessous est le code cr√©√© pour calculer la caract√©ristique d√©crite ci-dessus.""")
    # st.dataframe(price_df.head(3))
    st.write(""" Ici, nous :
    - Regroupons les prix hors pointe par entreprise et par mois
    - Obtenons les prix de janvier et de d√©cembre
    - Calculons la diff√©rence
    
    Nous obtenons """)

    st.dataframe(diff.head(3))
    st.subheader("Variation moyenne des prix sur les p√©riodes")
    st.write("""Nous pouvons maintenant am√©liorer la caract√©ristique  en calculant la 
    variation moyenne des prix sur des p√©riodes individuelles, plut√¥t que sur l'ensemble de l'ann√©e.""")
    st.write(""" Ici, nous :
    - Agr√©geons les prix moyens par p√©riode par entreprise
    - Calculons la diff√©rence moyenne entre les p√©riodes cons√©cutives : 'off_peak_peak_var_mean_diff', 
     'peak_mid_peak_var_mean_diff',  'off_peak_mid_peak_var_mean_diff',  'off_peak_peak_fix_mean_diff', 
     'peak_mid_peak_fix_mean_diff', 'off_peak_mid_peak_fix_mean_diff'
      """)
    code_agg = """# Aggregate average prices per period by company
    mean_prices = price_df.groupby(['id']).agg({
        'price_off_peak_var': 'mean',
        'price_peak_var': 'mean',
        'price_mid_peak_var': 'mean',
        'price_off_peak_fix': 'mean',
        'price_peak_fix': 'mean',
        'price_mid_peak_fix': 'mean'
    }).reset_index()

    # Calculate the mean difference between consecutive periods
    mean_prices['off_peak_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_peak_var']
    mean_prices['peak_mid_peak_var_mean_diff'] = mean_prices['price_peak_var'] - mean_prices['price_mid_peak_var']
    mean_prices['off_peak_mid_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices[
        'price_mid_peak_var']
    mean_prices['off_peak_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_peak_fix']
    mean_prices['peak_mid_peak_fix_mean_diff'] = mean_prices['price_peak_fix'] - mean_prices['price_mid_peak_fix']
    mean_prices['off_peak_mid_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices[
        'price_mid_peak_fix']
    columns = [
        'id',
        'off_peak_peak_var_mean_diff',
        'peak_mid_peak_var_mean_diff',
        'off_peak_mid_peak_var_mean_diff',
        'off_peak_peak_fix_mean_diff',
        'peak_mid_peak_fix_mean_diff',
        'off_peak_mid_peak_fix_mean_diff'
    ]
    df = pd.merge(df, mean_prices[columns], on='id')"""
    st.code(code_agg, language="python")

    st.write(""" Ces variables peuvent etre utiles, car elles ajoutent plus d'intensit√© aux variables de variations de
     prix cr√©es pr√©cedement. Au lieu de regarder les diff√©rences sur une ann√©e enti√®re, nous avons maintenant cr√©√©
      des variables qui examinent les diff√©rences moyennes de prix sur diff√©rentes p√©riodes
       (hors pointe, pointe, mi-pointe). La fonction d√©c-jan peut r√©v√©ler des tendances macro qui se produisent sur
        une ann√©e enti√®re, tandis que les variables inter-p√©riodes peuvent r√©v√©ler des tendances √† une √©chelle micro 
        entre les mois.""")

    st.subheader("Changements de prix maximaux √† travers les p√©riodes et les mois")
    st.write(""" Une autre fa√ßon dont nous pouvons am√©liorer la caract√©ristique  est d'examiner 
    le changement maximal de prix √† travers les p√©riodes et les mois.
     Nous :
     - Agr√©geons les prix moyens par p√©riode par entreprise
     - Calculons la diff√©rence moyenne entre les p√©riodes cons√©cutives
     - Calculons la diff√©rence mensuelle maximale √† travers les p√©riodes de temps
     """)

    code_price = """# Aggregate average prices per period by company
    mean_prices_by_month = price_df.groupby(['id', 'price_date']).agg({
        'price_off_peak_var': 'mean',
        'price_peak_var': 'mean',
        'price_mid_peak_var': 'mean',
        'price_off_peak_fix': 'mean',
        'price_peak_fix': 'mean',
        'price_mid_peak_fix': 'mean'
    }).reset_index()

    # Calculate the mean difference between consecutive periods
    mean_prices_by_month['off_peak_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - \
                                                          mean_prices_by_month['price_peak_var']
    mean_prices_by_month['peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_peak_var'] - mean_prices_by_month[
        'price_mid_peak_var']
    mean_prices_by_month['off_peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - \
                                                              mean_prices_by_month['price_mid_peak_var']
    mean_prices_by_month['off_peak_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - \
                                                          mean_prices_by_month['price_peak_fix']
    mean_prices_by_month['peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_peak_fix'] - mean_prices_by_month[
        'price_mid_peak_fix']
    mean_prices_by_month['off_peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - \
                                                              mean_prices_by_month['price_mid_peak_fix']
    # Calculate the maximum monthly difference across time periods
    max_diff_across_periods_months = mean_prices_by_month.groupby(['id']).agg({
        'off_peak_peak_var_mean_diff': 'max',
        'peak_mid_peak_var_mean_diff': 'max',
        'off_peak_mid_peak_var_mean_diff': 'max',
        'off_peak_peak_fix_mean_diff': 'max',
        'peak_mid_peak_fix_mean_diff': 'max',
        'off_peak_mid_peak_fix_mean_diff': 'max'
    }).reset_index().rename(
        columns={
            'off_peak_peak_var_mean_diff': 'off_peak_peak_var_max_monthly_diff',
            'peak_mid_peak_var_mean_diff': 'peak_mid_peak_var_max_monthly_diff',
            'off_peak_mid_peak_var_mean_diff': 'off_peak_mid_peak_var_max_monthly_diff',
            'off_peak_peak_fix_mean_diff': 'off_peak_peak_fix_max_monthly_diff',
            'peak_mid_peak_fix_mean_diff': 'peak_mid_peak_fix_max_monthly_diff',
            'off_peak_mid_peak_fix_mean_diff': 'off_peak_mid_peak_fix_max_monthly_diff'
        }
    )
    columns = [
        'id',
        'off_peak_peak_var_max_monthly_diff',
        'peak_mid_peak_var_max_monthly_diff',
        'off_peak_mid_peak_var_max_monthly_diff',
        'off_peak_peak_fix_max_monthly_diff',
        'peak_mid_peak_fix_max_monthly_diff',
        'off_peak_mid_peak_fix_max_monthly_diff'
    ]

    df = pd.merge(df, max_diff_across_periods_months[columns], on='id')"""
    st.code(code_price, language="python")

    st.write(""" Nous avons pener que calculer la variation maximale des prix entre les mois et les p√©riodes serait
     une bonne caract√©ristique √† cr√©er, car j'essayais de r√©fl√©chir du point de vue d'un client de PowerCo. 
     En tant que client des services publics, rien n'est plus aga√ßant que des changements soudains de prix entre les
      mois, et une forte augmentation des prix sur une courte p√©riode serait un facteur influent me poussant √† examiner
       d'autres fournisseurs de services publics pour une meilleure offre. Puisque nous essayons de pr√©dire le 
       d√©sabonnement pour ce cas d'utilisation, j'ai pens√© que ce serait une caract√©ristique int√©ressante 
       √† inclure. """)

    st.subheader("Transformations des autres variables assymetriques  ")
    st.write(""" Cette section aborde la transformation des variables suppl√©mentaires auxquelles nous avons peut-√™tre
     pens√©, ainsi que diff√©rentes fa√ßons de transformer nos donn√©es pour prendre en compte certaines de leurs 
     propri√©t√©s statistiques que nous avons vues pr√©c√©demment, telles que la sym√©trie.""")
    st.markdown("##### Tenure")
    st.write(""" Pendant combien de temps une entreprise est-elle cliente de PowerCo ?""")
    st.dataframe(df.groupby(['tenure']).agg({'churn': 'mean'}).sort_values(by='churn', ascending=False))
    st.write(""" Nous pouvons voir que les entreprises qui sont clientes depuis seulement 4 mois ou moins sont beaucoup
     plus susceptibles de se d√©sabonner par rapport aux entreprises qui sont clientes depuis plus longtemps.
      Et donc, la diff√©rence entre 4 et 5 mois est d'environ 4 %, ce qui repr√©sente un saut important dans la 
      probabilit√© qu'un client se d√©sabonne par rapport aux autres diff√©rences entre les valeurs ordonn√©es de la dur√©e.
       Cela r√©v√®le peut-√™tre que le fait d'amener un client au-del√† de 4 mois de dur√©e est en r√©alit√© une √©tape 
       importante pour le maintenir en tant que client √† long terme.
       
       C'est une caract√©ristique int√©ressante √† conserver pour la mod√©lisation car il est clair que la dur√©e pendant 
       laquelle vous √™tes client influence la probabilit√© de d√©sabonnement du client.""")

    st.markdown(" ##### Transformer les dates en mois ")
    st.write(""""
   -  months_activ = Nombre de mois actifs jusqu'√† la date de r√©f√©rence (Jan 2016)
   -  months_to_end = Nombre de mois de contrat restants jusqu'√† la date de r√©f√©rence (Jan 2016)
   -  months_modif_prod = Nombre de mois depuis la derni√®re modification jusqu'√† la date de r√©f√©rence (Jan 2016)
   -  months_renewal = Nombre de mois depuis le dernier renouvellement jusqu'√† la date de r√©f√©rence (Jan 2016)""")


    code_months = """# Create reference date
    reference_date = datetime(2016, 1, 1)

    # Create columns
    df['months_activ'] = convert_months(reference_date, df, 'date_activ')
    df['months_to_end'] = -convert_months(reference_date, df, 'date_end')
    df['months_modif_prod'] = convert_months(reference_date, df, 'date_modif_prod')
    df['months_renewal'] = convert_months(reference_date, df, 'date_renewal')"""
    st.code(code_months, language="python")

    st.write(""" Les dates sous forme datetime ne sont pas utiles pour un mod√®le pr√©dictif, nous devions donc utiliser
     ces dates pour cr√©er d'autres caract√©ristiques qui pourraient avoir une certaine puissance pr√©dictive.
      En utilisant l'intuition, on pourrait supposer qu'un client qui est client actif de PowerCo depuis plus 
      longtemps pourrait avoir plus de fid√©lit√© √† la marque et est plus susceptible de rester. 
      Alors qu'un client plus r√©cent pourrait √™tre plus volatile. D'o√π l'ajout de la caract√©ristique months_activ.

      De plus, si nous pensons du point de vue d'un client de PowerCo, si vous approchez de la fin de votre contrat
       avec PowerCo, vos pens√©es pourraient aller dans quelques directions. 
       Vous pourriez rechercher de meilleures offres pour quand votre contrat se termine, ou vous pourriez vouloir 
       voir votre contrat actuel jusqu'√† son terme et en signer un autre. D'un autre c√¥t√©, si vous venez de vous 
       joindre, vous pourriez avoir une p√©riode o√π vous √™tes autoris√© √† partir si vous n'√™tes pas satisfait. 
       De plus, si vous √™tes au milieu de votre contrat, il pourrait y avoir des frais si vous vouliez partir, 
       dissuadant les clients de se d√©sabonner au milieu de leur accord. Ainsi, je pense que months_to_end sera une 
       caract√©ristique int√©ressante car elle peut r√©v√©ler des sch√©mas et des comportements concernant le moment
        du d√©sabonnement.

        Je crois que si un client a apport√© des mises √† jour r√©centes √† son contrat, il est plus susceptible d'√™tre
         satisfait ou du moins il a re√ßu un niveau de service client pour mettre √† jour ou modifier ses services
          existants. Je crois que cela est un signe positif, montrant qu'ils sont un client engag√©, et donc je pense 
          que months_modif_prod sera une caract√©ristique int√©ressante √† inclure car elle montre le degr√© d'engagement 
          d'un client avec PowerCo.

     Enfin, le nombre de mois depuis la derni√®re fois qu'un client a renouvel√© un contrat sera, √† mon avis, 
     une caract√©ristique int√©ressante car une fois de plus, elle montre le degr√© d'engagement de ce client. Cela va 
     √©galement plus loin que l'engagement, montrant un niveau d'engagement si un client renouvelle son contrat.
      Pour cette raison, je pense que months_renewal sera une bonne caract√©ristique √† inclure. """)
    st.write(""" Nous n'avons plus besoin des colonnes datetime que nous avons utilis√©es pour les transformations,
     nous pouvons donc les supprimer.""")
    code_remove = """remove = [
        'date_activ',
        'date_end',
        'date_modif_prod',
        'date_renewal'
    ]

    df = df.drop(columns=remove)"""
    st.code(code_remove, language="python")
    st.subheader("Transformation des donn√©es bool√©ennes")
    st.write("has_gas")
    st.write("Nous voulons simplement transformer cette colonne de cat√©gorique en un indicateur binaire.")
    st.dataframe(df.groupby(['has_gas']).agg({'churn': 'mean'}))
    st.write(""" Si un client ach√®te √©galement du gaz chez PowerCo, cela montre qu'il poss√®de plusieurs produits 
    et est un client fid√®le √† la marque. Il n'est donc pas surprenant que les clients qui n'ach√®tent pas de gaz 
    soient presque 2 % plus susceptibles de r√©silier leur contrat que les clients qui ach√®tent √©galement du gaz 
    chez PowerCo. C'est donc une caract√©ristique utile.""")
    st.subheader("Transformer les donn√©es cat√©gorielles")
    st.markdown(""" Un mod√®le pr√©dictif ne peut pas accepter des valeurs cat√©gorielles ou de cha√Æne, donc en tant que
     data scientist, nous devons encoder les caract√©ristiques cat√©gorielles en repr√©sentations num√©riques de la
      mani√®re la plus compacte et discriminative possible. La m√©thode la plus simple consiste √† mapper chaque cat√©gorie
       sur un entier (encodage par √©tiquette), cependant cela n'est pas toujours appropri√© car cela introduit ensuite
        le concept d'ordre dans une caract√©ristique qui peut ne pas √™tre intrins√®quement pr√©sente 0 < 1 < 2 < 3 ...

      Une autre fa√ßon d'encoder les caract√©ristiques cat√©gorielles est d'utiliser des variables factices,
       √©galement appel√©es encodage one-hot. Cela cr√©e une nouvelle caract√©ristique pour chaque valeur unique d'une 
       colonne cat√©gorielle et remplit cette colonne avec un 1 ou un 0 pour indiquer si cette entreprise appartient
        ou non √† cette cat√©gorie..""")
    st.markdown("##### channel_sales")

    # Let's see how many categories are within this column
    st.dataframe(client_df['channel_sales'].value_counts())
    st.write("""
    Nous avons 8 cat√©gories, nous allons donc cr√©er 8 variables factices √† partir de cette colonne. Cependant, comme
     nous pouvons le voir pour les trois derni√®res cat√©gories dans la sortie ci-dessus, elles ont respectivement 
     11, 3 et 2 occurrences. √âtant donn√© que notre ensemble de donn√©es compte environ 14 000 lignes,
      cela signifie que ces variables factices seront presque enti√®rement √©gales √† 0 et n'ajouteront 
      donc pas beaucoup de puissance pr√©dictive au mod√®le (car elles sont presque enti√®rement constantes 
      et fournissent tr√®s peu d'information).
     Pour cette raison, nous allons supprimer ces 3 variables factices.""")

    code_dummies ="""df = pd.get_dummies(df, columns=['channel_sales'], prefix='channel')
    df = df.drop(columns=['channel_sddiedcslfslkckwlfkdpoeeailfpeds', 'channel_epumfxlbckeskwekxbiuasklxalciiuu',
                          'channel_fixdbufsefwooaasfcxdxadsiekoceaa'])"""
    st.code(code_dummies, language="python")
    st.markdown("##### origin_up")
    # Let's see how many categories are within this column
    st.dataframe(client_df['origin_up'].value_counts())
    st.write(""" De mani√®re similaire √† `channel_sales`, les trois derni√®res cat√©gories dans la sortie ci-dessus
     montrent une fr√©quence tr√®s faible. Nous allons donc les supprimer des caract√©ristiques apr√®s la cr√©ation 
     des variables factices. """)

    st.subheader("Transformation des donn√©es num√©riques")
    st.markdown(""" Dans la partie pr√©dedente, nous avons constat√© que certaines variables √©taient fortement 
    asym√©triques. La raison pour laquelle nous devons traiter l'asym√©trie est que certains mod√®les pr√©dictifs ont 
    des hypoth√®ses inh√©rentes sur la distribution des caract√©ristiques qui leur sont fournies. 
    Ces mod√®les sont appel√©s mod√®les param√©triques et ils supposent g√©n√©ralement que toutes les variables sont √† 
    la fois ind√©pendantes et normalement distribu√©es.

L'asym√©trie n'est pas toujours une mauvaise chose, mais en r√®gle g√©n√©rale, il est toujours bon de traiter 
les variables fortement asym√©triques pour les raisons mentionn√©es ci-dessus, mais aussi parce que cela peut
 am√©liorer la vitesse √† laquelle les mod√®les pr√©dictifs convergent vers leur meilleure solution.

Il existe de nombreuses fa√ßons de traiter les variables asym√©triques. Nous pouvons appliquer des transformations
 telles que:
  - la racine carr√©e,
  - la racine cubique,
  - le logarithme √† une colonne num√©rique continue.
  
   Pour ce projet, nous utiliserons la transformation du "logarithme" pour les caract√©ristiques positivement
    asym√©triques.

    Note : Nous ne pouvons pas appliquer le logarithme √† une valeur de 0, nous ajouterons donc une constante de 1 
    √† toutes les valeurs.
    
    Tout d'abord, je veux voir les statistiques des caract√©ristiques asym√©triques, afin que nous puissions
     comparer avant et apr√®s la transformation.""")


    st.dataframe(skewed_before)

    st.subheader("Nous pouvons constater que l'√©cart-type pour la plupart de ces caract√©ristiques est assez √©lev√©.")
    # Apply log10 transformation
    st.dataframe(skewed_after)

    st.markdown("""
    Maintenant, nous pouvons voir que pour la majorit√© des caract√©ristiques, leur √©cart type est beaucoup plus bas 
    apr√®s la transformation. C'est une bonne chose, cela montre que ces caract√©ristiques sont maintenant plus 
    stables et pr√©visibles. V√©rifions rapidement les distributions de certaines de ces caract√©ristiques √©galement.""")

    col1, col2, col3 = st.columns(3)
    with col1:
        fig, ax = plt.subplots(figsize=(18, 20))
        sns.distplot((df["cons_12m"].dropna()))
        st.pyplot(fig)
    with col2:
        fig, ax = plt.subplots(figsize=(18, 20))
        sns.distplot((df[df["has_gas"] == 1]["cons_gas_12m"].dropna()))
        st.pyplot(fig)

    with col3:
        fig, axs = plt.subplots(figsize=(18, 20))
        # Plot histograms
        sns.distplot((df["cons_last_month"].dropna()))
        st.pyplot(fig)

    st.subheader("Correlations")
    st.markdown("""
    En ce qui concerne la cr√©ation de nouvelles variables et la transformation de celles existantes, c'est vraiment 
    une situation d'essais et d'erreurs qui n√©cessite de l'it√©ration. Une fois que nous formons un mod√®le pr√©dictif,
     nous pouvons voir quelles variables fonctionnent ou non, et nous saurons √©galement √† quel point cet ensemble
      de fonctionnalit√©s est pr√©dictif. Sur cette base, nous pouvons revenir √† l'ing√©nierie des donn√©es pour 
      am√©liorer notre mod√®le.

      Pour l'instant, nous laisserons les l'ing√©nierie de donn√©es √† ce stade. Une autre chose toujours utile √† 
      examiner est la corr√©lation entre toutes les variables de votre ensemble de donn√©es.
     
     C'est important car cela r√©v√®le les relations lin√©aires entre les fonctionnalit√©s. Nous voulons que
      les variables soient corr√©l√©es avec le d√©sabonnement, car cela indiquera qu'elles sont de bons pr√©dicteurs. 
      Cependant, des variables ayant une corr√©lation tr√®s √©lev√©e peuvent parfois √™tre suspectes. 
      Cela est d√ª au fait que deux colonnes fortement corr√©l√©es indiquent qu'elles peuvent partager beaucoup des 
      m√™mes informations. L'une des hypoth√®ses de tout mod√®le pr√©dictif param√©trique (comme indiqu√© pr√©c√©demment) est 
      que toutes les variables doivent √™tre ind√©pendantes.

     Id√©alement, nous voulons un ensemble de variables ayant une corr√©lation de 0 avec toutes les variables 
     ind√©pendantes (toutes les variables sauf notre variable cible) et une corr√©lation √©lev√©e avec la variable 
     cible (d√©sabonnement). Cependant, c'est tr√®s rarement le cas et il est courant d'avoir un faible degr√© de 
     corr√©lation entre les variables ind√©pendantes.

     Regardons maintenant comment toutes les variables du mod√®le sont corr√©l√©es. """)

    df_copy = df.drop(['Unnamed: 0', 'id', 'price_date_x', 'price_date_y', 'price_date'], axis=1)
    # display_data_types_info(df)
    # st.dataframe(df.head(3))
    correlation = df_copy.corr()

    # Plot correlation

    fig, ax = plt.subplots(figsize=(45, 45))
    sns.heatmap(
        correlation,
        xticklabels=correlation.columns.values,
        yticklabels=correlation.columns.values,
        annot=True,
        annot_kws={'size': 12}
    )
    # Axis ticks size
    plt.xticks(fontsize=15)
    plt.yticks(fontsize=15)
    st.pyplot(fig)

    st.write(""" Nous allons supprimer deux variables qui pr√©sentent une corr√©lation √©lev√©e avec d'autres
     variables ind√©pendantes : 'num_years_antig' et 'forecast_cons_year'.""")

if selected_option == "Mod√©lisation":

    st.write(""" Nous avons maintenant un ensemble de donn√©es contenant des caract√©ristiques que nous avons con√ßues et 
    nous sommes pr√™ts √† commencer l'entra√Ænement d'un mod√®le pr√©dictif. Nous nous concentrer uniquement
     sur l'entra√Ænement d'un Random Forest. """)
    st.subheader("  L'√©chantillonnage de donn√©es ")
    st.write(""" La premi√®re chose que nous voulons faire est de diviser notre ensemble de donn√©es en √©chantillons
     d'entra√Ænement et de test.""")
    code_model =""" 
    df_copy = df.drop(['Unnamed: 0', 'id', 'price_date_x', 'price_date_y', 'price_date'], axis=1)
    train_df = df_copy.copy()

    # Separate target variable from independent variables
    y = df['churn']
    X = df.drop(columns=['id', 'churn', 'Unnamed: 0', 'price_date_x', 'price_date_y', 'price_date'])
    print(X.shape)
    print(y.shape)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"""
    st.code(code_model, language="python")

    st.subheader("Model training")
    st.markdown("""
    Encore une fois, nous utilisons un classificateur de for√™t al√©atoire dans cet exemple. Une for√™t al√©atoire appartient √† la cat√©gorie des algorithmes d'ensemble car, en interne, la for√™t fait r√©f√©rence √† une collection d'arbres de d√©cision qui sont des algorithmes d'apprentissage bas√©s sur les arbres. En tant que data scientist, vous pouvez contr√¥ler la taille de la for√™t (c'est-√†-dire le nombre d'arbres de d√©cision que vous souhaitez inclure).

La raison pour laquelle un algorithme d'ensemble est puissant r√©side dans les lois de la moyenne, les apprenants faibles et le th√©or√®me central limite. Si nous prenons un seul arbre de d√©cision et lui donnons un √©chantillon de donn√©es et certains param√®tres, il apprendra des motifs √† partir des donn√©es. Il peut √™tre surajust√© ou sous-ajust√©, mais c'est maintenant notre seul espoir, ce mod√®le unique.

Avec les m√©thodes d'ensemble, au lieu de compter sur un seul mod√®le entra√Æn√©, nous pouvons former des milliers d'arbres de d√©cision, tous utilisant des d√©coupages diff√©rents des donn√©es et apprenant diff√©rents motifs. Cela reviendrait √† demander √† 1000 personnes d'apprendre toutes √† coder. Vous finiriez avec 1000 personnes ayant diff√©rentes r√©ponses, m√©thodes et styles ! La notion d'apprenant faible s'applique √©galement ici. On a d√©couvert que si vous entra√Ænez vos apprenants √† ne pas surajuster, mais √† apprendre de faibles motifs dans les donn√©es et que vous avez beaucoup de ces apprenants faibles, ensemble, ils se r√©unissent pour former une piscine de connaissances hautement pr√©dictive ! C'est une application de la vie r√©elle de la notion que plusieurs cerveaux valent mieux qu'un seul.

Maintenant, au lieu de compter sur un seul arbre de d√©cision pour la pr√©diction, la for√™t al√©atoire le soumet aux opinions globales de l'ensemble complet des arbres de d√©cision. Certains algorithmes d'ensemble utilisent une approche de vote pour d√©cider de la meilleure pr√©diction, d'autres utilisent une moyenne.

√Ä mesure que nous augmentons le nombre d'apprenants, l'id√©e est que les performances de la for√™t al√©atoire devraient converger vers sa meilleure solution possible.

Certains avantages suppl√©mentaires du classificateur de for√™t al√©atoire incluent :

- La for√™t al√©atoire utilise une approche bas√©e sur des r√®gles au lieu d'un calcul de distance, et donc les fonctionnalit√©s n'ont pas besoin d'√™tre mises √† l'√©chelle.
- Elle est capable de g√©rer les param√®tres non lin√©aires mieux que les mod√®les bas√©s sur la lin√©arit√©.
Cependant, certains inconv√©nients du classificateur de for√™t al√©atoire comprennent :

- La puissance de calcul n√©cessaire pour entra√Æner une for√™t al√©atoire sur un grand ensemble de donn√©es est √©lev√©e, car nous devons construire tout un ensemble d'estimateurs.
- Le temps d'entra√Ænement peut √™tre plus long en raison de la complexit√© et de la taille accrue de l'ensemble.
    """)
    code_model2= """
    model = RandomForestClassifier(
        n_estimators=1000
    )
    model.fit(X_train, y_train)"""
    st.code(code_model2, language="python")
    st.subheader("Evaluation")
    st.markdown(""" √âvaluons maintenant √† quel point ce mod√®le entra√Æn√© est capable de pr√©dire les valeurs de l'ensemble de donn√©es de test.

Nous allons utiliser 3 m√©triques pour √©valuer les performances :

- Pr√©cision = le rapport des observations correctement pr√©dites sur l'ensemble des observations
- Pr√©cision = la capacit√© du classificateur √† ne pas √©tiqueter un √©chantillon n√©gatif comme positif
- Rappel = la capacit√© du classificateur √† trouver tous les √©chantillons positifs

La raison pour laquelle nous utilisons ces trois m√©triques est que la simple pr√©cision n'est pas toujours une bonne mesure √† utiliser. Pour donner un exemple, supposons que vous pr√©disez des insuffisances cardiaques chez des patients √† l'h√¥pital et qu'il y avait 100 patients sur 1000 qui avaient une insuffisance cardiaque.

Si vous avez pr√©dit correctement 80 sur 100 (80%) des patients qui avaient effectivement une insuffisance cardiaque, vous pourriez penser que vous avez bien fait ! Cependant, cela signifie √©galement que vous avez pr√©dit 20 erreurs et quelles pourraient √™tre les implications de pr√©dire ces 20 patients restants de mani√®re incorrecte ? Peut-√™tre qu'ils passent √† c√¥t√© d'un traitement vital pour sauver leur vie.

De plus, quelle est l'impact de pr√©dire des cas n√©gatifs comme positifs (personnes n'ayant pas une insuffisance cardiaque mais √©tant pr√©dites positives) ? Peut-√™tre qu'un grand nombre de faux positifs signifie que des ressources sont utilis√©es pour les mauvaises personnes et beaucoup de temps est gaspill√© alors qu'ils auraient pu aider les v√©ritables personnes souffrant d'insuffisance cardiaque.

Il s'agit simplement d'un exemple, mais il illustre pourquoi d'autres m√©triques de performance sont n√©cessaires, telles que la pr√©cision et le rappel, qui sont de bonnes mesures √† utiliser dans un sc√©nario de classification..""")


    st.write(f"True positives: {tp}")
    st.write(f"False positives: {fp}")
    st.write(f"True negatives: {tn}")
    st.write(f"False negatives: {fn}\n")

    st.write(f"Accuracy: {metrics.accuracy_score(y_test, predictions)}")
    st.write(f"Precision: {metrics.precision_score(y_test, predictions)}")
    st.write(f"Recall: {metrics.recall_score(y_test, predictions)}")

    st.markdown(""" En examinant ces r√©sultats, quelques points √† souligner :

Remarque : Si vous ex√©cutez ce notebook vous-m√™me, vous pourriez obtenir des r√©ponses l√©g√®rement diff√©rentes !

Dans l'ensemble du jeu de test, environ 10% des lignes correspondent √† des clients r√©siliants (churn = 1).
En ce qui concerne les vrais n√©gatifs, nous en avons 3282 sur 3286. Cela signifie que sur l'ensemble des cas n√©gatifs (churn = 0), nous avons pr√©dit correctement 3282 comme n√©gatifs (d'o√π le nom de vrais n√©gatifs). C'est excellent !
En ce qui concerne les faux n√©gatifs, cela correspond aux cas o√π nous avons pr√©dit qu'un client ne r√©silierait pas (churn = 0) alors qu'en r√©alit√© il a r√©sili√© (churn = 1). Ce nombre est assez √©lev√© √† 348, nous voulons r√©duire autant que possible le nombre de faux n√©gatifs, donc cela devrait √™tre abord√© lors de l'am√©lioration du mod√®le.
En ce qui concerne les faux positifs, cela correspond aux cas o√π nous avons pr√©dit qu'un client r√©silierait alors qu'en r√©alit√© il n'a pas r√©sili√©. Pour cette valeur, nous pouvons voir qu'il y a 4 cas, ce qui est excellent !
Pour les vrais positifs, nous pouvons voir qu'au total, nous avons 366 clients qui ont r√©sili√© dans l'ensemble de donn√©es de test. Cependant, nous ne sommes capables d'identifier correctement que 18 de ces 366, ce qui est tr√®s faible.
En examinant le score de pr√©cision, il est tr√®s trompeur ! C'est pourquoi l'utilisation de la pr√©cision et du rappel est importante. Le score de pr√©cision est √©lev√©, mais il ne nous dit pas toute l'histoire.
En examinant le score de rappel, cela montre un score de 0,82, ce qui n'est pas mal, mais pourrait √™tre am√©lior√©.
Cependant, le rappel montre que le classificateur a une tr√®s faible capacit√© √† identifier les √©chantillons positifs. Cela serait la principale pr√©occupation pour l'am√©lioration de ce mod√®le !
En r√©sum√©, nous sommes capables d'identifier tr√®s pr√©cis√©ment les clients qui ne r√©silient pas, mais nous ne sommes pas capables de pr√©dire les cas o√π les clients r√©silient ! Ce que nous constatons, c'est qu'un pourcentage √©lev√© de clients sont identifi√©s comme ne r√©siliant pas alors qu'ils devraient √™tre identifi√©s comme r√©siliant. Cela me fait penser que l'ensemble actuel de fonctionnalit√©s n'est pas assez discriminant pour distinguer clairement les r√©siliants des non-r√©siliants.

Un data scientist √† ce stade reviendrait √† l'ing√©nierie des fonctionnalit√©s pour essayer de cr√©er des fonctionnalit√©s plus pr√©dictives. Il pourrait √©galement exp√©rimenter avec l'optimisation des param√®tres dans le mod√®le pour am√©liorer les performances. Pour l'instant, plongeons un peu plus dans la compr√©hension du mod√®le. """)
    st.subheader("Interpretation du mod√®le")
    st.markdown("""
    Une mani√®re simple de comprendre les r√©sultats d'un mod√®le est d'examiner les importances des fonctionnalit√©s. 
    Les importances des fonctionnalit√©s indiquent l'importance d'une fonctionnalit√© dans le mod√®le pr√©dictif. 
    Il existe plusieurs fa√ßons de calculer l'importance des fonctionnalit√©s, mais avec le classificateur Random Forest,
     nous sommes en mesure d'extraire les importances des fonctionnalit√©s √† l'aide de la m√©thode int√©gr√©e dans 
     le mod√®le entra√Æn√©. Dans le cas du Random Forest, l'importance des fonctionnalit√©s repr√©sente le nombre de fois 
     que chaque fonctionnalit√© est utilis√©e pour diviser l'ensemble des arbres.
    """)

    fig, ax = plt.subplots(figsize=(15, 25))
    plt.title('Feature Importances')
    plt.barh(range(len(feature_importances)), feature_importances['importance'], color='b', align='center')
    plt.yticks(range(len(feature_importances)), feature_importances['features'])
    plt.xlabel('Importance')
    st.pyplot(fig)

    st.markdown(""" √Ä partir de ce graphique, nous pouvons observer les points suivants :

- La marge nette et la consommation sur 12 mois sont des facteurs cl√©s de l'attrition dans ce mod√®le.
- La marge sur l'abonnement d'√©lectricit√© est √©galement un facteur influent.
- Le temps semble √™tre un facteur influent, en particulier le nombre de mois d'activit√©, la dur√©e de leur relation contractuelle et le nombre de mois depuis leur derni√®re mise √† jour de contrat.
- La caract√©ristique recommand√©e par notre coll√®gue se situe dans la moiti√© sup√©rieure en termes d'influence, et certaines des caract√©ristiques d√©velopp√©es √† partir de celle-ci la surpassent effectivement.
- Nos caract√©ristiques de sensibilit√© au prix sont dispers√©es, mais ne sont pas le principal moteur du d√©part d'un client.

La derni√®re observation est importante car elle renvoie √† notre hypoth√®se initiale : """)

    st.subheader(" L'attrition est-elle motiv√©e par la sensibilit√© au prix des clients ?")
    st.markdown(""" D'apr√®s les r√©sultats des importances des fonctionnalit√©s, ce n'est pas un facteur principal,
     mais plut√¥t un contributeur faible. Cependant, pour parvenir √† une conclusion d√©finitive, davantage 
     d'exp√©rimentation est n√©cessaire.""")
    code_pred =""" proba_predictions = model.predict_proba(X_test)
    probabilities = proba_predictions[:, 1]
    X_test = X_test.reset_index()
    X_test.drop(columns='index', inplace=True)
    X_test['churn'] = predictions.tolist()
    X_test['churn_probability'] = probabilities.tolist()"""
    st.code(code_pred, language="python")

if selected_option == "Impact Business de la Remise de 20%" :
    st.markdown(""" Nous effectuons une analyse de la strat√©gie de remise propos√©e. L'entreprise a propos√© d'offrir 
    une remise de 20% aux clients ayant une forte propension √† la r√©siliation. Nous pouvons supposer, pour commencer,
     que tous ceux √† qui une remise est offerte l'accepteront.""")
    st.markdown(""" Notre t√¢che est de calculer le chiffre d'affaires pr√©vu pour l'ensemble des clients :
    - Lorsqu'aucune remise n'est propos√©e
    - Lorsqu'une remise est offerte en fonction d'un seuil de probabilit√© pour d√©cider qui devrait recevoir la remise de 20%
    - Ainsi, d√©cider o√π fixer le seuil pour maximiser le chiffre d'affaires.""")

    st.subheader("Calculons une estimation de revenus de base")
    st.markdown(" Calculons une estimation du revenu de l'√©lectricit√© pour chaque client au cours des douze prochains"
                " mois, sur la base de la consommation pr√©vue, du prix pr√©vu et du taux de d√©sabonnement r√©el."
                " Appelons cette estimation basecase_revenue.")

    st.markdown(""" Pour les clients qui finissent par r√©silier, nous devrions r√©duire notre calcul de chiffre 
    d'affaires pr√©vu de 91,9 % pour tenir compte des clients r√©siliant entre janvier 2016 et le d√©but de mars 2016. 
    (Ne sachant pas quand ils r√©silient, une hypoth√®se raisonnable pour la perte de revenus est la moyenne de 100 %,
     correspondant √† la r√©siliation le 1er janvier 2016, et 83,9 %, correspondant √† la r√©siliation √† la fin de f√©vrier,
      soit 59 jours sur une ann√©e de 365 jours). Appelez cette nouvelle variable "basecase_revenue_after_churn", 
      c'est-√†-dire "basecase_revenue_after_churn = basecase_revenue * (1 - 0,919 * churn)".""")

    st.dataframe(df_revenu)
    st.subheader("Calculons les avantages et les co√ªts estim√©s de l'intervention")
    st.markdown(""" Maintenant, choisissez une probabilit√© de seuil (par exemple, 0,5) de mani√®re √† ce que :
    - Les clients avec une probabilit√© de r√©siliation plus √©lev√©e que le seuil obtiennent une remise, et
    - Les clients en dessous de la probabilit√© de r√©siliation ne b√©n√©ficient pas d'une remise.
    
    √Ä partir de cela, calculez les revenus du sc√©nario d'intervention en supposant que :
    - Tous les clients √† qui une remise est propos√©e l'acceptent.
    - Les clients qui re√ßoivent une remise sont cens√©s ne pas r√©silier au cours des douze prochains mois (c'est-√†-dire
     une probabilit√© de r√©siliation de 0), et donc le revenu retenu est de 0,8 * basecase_revenue, 
     soit (1 - fraction_de_remise) * basecase_revenue.
    - Les clients qui ne re√ßoivent pas de remise sont suppos√©s r√©silier en fonction de la variable d√©pendante observ√©e
     (c'est-√†-dire un 1 ou un 0 pour savoir s'ils ont effectivement r√©sili√© ou non).
     Maintenant, tracez la variation des revenus en fonction de la probabilit√© de seuil sur un graphique. Quelle 
     probabilit√©  de seuil optimise approximativement le r√©sultat financier ? Supposons, pour ces calculs, que le 
     client ne consomme pas plus ou moins d'√©lectricit√© en raison des changements de prix. En pratique, nous nous 
     attendrions √† ce que si le co√ªt du client diminue, sa consommation puisse augmenter. Nous verrons deux effets
      compensatoires en jeu :
      - Pour les vrais positifs, nous verrons la r√©tention de revenus par rapport au sc√©nario sans remise.
      - Pour les faux positifs, nous verrons une r√©duction des revenus en leur accordant une remise alors qu'ils 
      ne r√©silieraient pas en r√©alit√©.
      (Les faux n√©gatifs repr√©sentent un co√ªt d'opportunit√© mais pas une diff√©rence de co√ªt r√©el entre les deux
       sc√©narios.)
       Le point de coupure optimal √©quilibrera les avantages des vrais positifs par rapport aux co√ªts des faux 
       positifs. Notre t√¢che est de trouver approximativement le point de coupure optimal. Nous pourrions avoir besoin 
       de faire des hypoth√®ses suppl√©mentaires. Si nous pensons que les hypoth√®ses ci-dessus ne sont pas justifi√©es 
       et que d'autres sont meilleures, nous devrions modifier nos hypoth√®ses.""")
    plot_tradeoff(rev_deltas)

    st.subheader("comment choisir le seuil de coupure ?")
    st.markdown(""" Ci-dessus, nous avons d√©cid√© √† qui offrir la r√©duction en fonction d'un seuil de probabilit√©. 
    Est-ce la strat√©gie optimale ? 
    - Par exemple, nous pourrions offrir des r√©ductions √† des clients peu rentables, 
    aggravant ainsi consid√©rablement nos marges globales. Par exemple, si l'offre d'une r√©duction rend le client non 
    rentable en termes de marge nette, nous pourrions pr√©f√©rer les laisser partir plut√¥t que de les sauver.
    - M√™me si nous ne consid√©rons que les revenus, cette strat√©gie pourrait ne pas √™tre optimale d'un point de vue 
    financier. Nous pouvons calculer l'impact attendu sur les revenus de notre strat√©gie et donner la priorit√© aux
     clients pour lesquels la probabilit√© de churn est √©lev√©e, mais qui peuvent √©galement √™tre des clients pr√©cieux.
     
     Un principe g√©n√©ral ici est que nous pouvons nous permettre de d√©penser davantage pour conserver les clients √† 
     forte valeur, car les co√ªts de les perdre sont plus √©lev√©s. Une erreur tr√®s courante dans les applications
      commerciales de la pr√©vention du churn est de se concentrer sur la probabilit√© de churn en oubliant l'impact sur 
      la valeur (dans une plus ou moins grande mesure). Nous avons constat√© de nombreux cas o√π nos clients consacrent 
      autant d'efforts √† la fid√©lisation de clients peu rentables qu'√† la fid√©lisation de clients tr√®s rentables. """)

    # Generate a list of possible cutoffs and the corresponding overall revenue deltas
    plot_tradeoff(rev_deltas_high_value)

    st.markdown(""" 
    **Note:**
    In this case, it doesn't make sense to prioritize large-revenue customers, since the overall revenue delta is much 
    lower than when targeting everyone. However, this is only the case here since the intervention doesn't depend on 
    the number of customers (simply adjusting prices). The interventions usually go beyond simply adjusting prices to 
    prevent churn.
    There may be the option of intensifying the customer relation, adding key account managers, or other interventions 
    that do incur costs depending on how many customers are targeted. In that case, it may be benefitial to target only 
    a subset of customers to save on these costs, even if the delta in the figure above is reduced.
    """)
    st.subheader(" Utilisons les pr√©visions plut√¥t que les churns r√©els")

    fig = plt.figure(figsize=(10, 10))
    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
    ax2 = plt.subplot2grid((3, 1), (2, 0))
    ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_true=test_df['churn'],
        y_prob=test_df['churn_probability'],
        n_bins=10
    )
    ax1.plot(mean_predicted_value, fraction_of_positives, "s-", label="Our RF classifier")
    ax2.hist(test_df['churn_probability'], range=(0, 1), bins=10, histtype="step", lw=2)
    ax1.set_ylabel("Fraction of positives")
    ax1.set_ylim([-0.05, 1.05])
    ax1.legend(loc="lower right")
    ax1.set_title('Calibration plots (reliability curve)')
    ax2.set_xlabel("Mean predicted value")
    ax2.set_ylabel("Count")
    plt.tight_layout()
    st.pyplot(fig)

    st.markdown(""" Cette repr√©sentation graphique nous indique quelques √©l√©ments :
    1. La courbe de calibration sup√©rieure pr√©sente une courbe sigmo√Øde, ce qui est typique pour un classifieur 
    sous-confiant.
    2. Le graphique inf√©rieur nous montre que le mod√®le est positivement biais√© vers la pr√©diction d'une probabilit√©,
     peut-√™tre en raison d'une confiance tr√®s faible.
    En pratique, quelques ajustements seraient n√©cessaires pour calibrer les probabilit√©s, mais √† des fins de 
    d√©monstration, nous allons passer outre √† cette √©tape.""")
    plot_tradeoff(rev_deltas)

    st.subheader("Comment choisir la remise ?")
    st.markdown(""" Dans la strat√©gie sugg√©r√©e par le responsable de la division PME, nous offrons une remise de 20 % √† tous les clients cibl√©s. Cependant, cela pourrait ne pas √™tre optimal non plus. Nous avons suppos√© auparavant que les clients √† qui une remise est offerte ne r√©silieront pas. Cependant, cela pourrait ne pas √™tre vrai en r√©alit√©. La remise pourrait ne pas √™tre suffisamment importante pour √©viter la r√©siliation.

En fait, nous pouvons pr√©dire la probabilit√© de r√©siliation pour chaque client en fonction du prix, de la marge et d'autres facteurs. Par cons√©quent, nous pouvons essayer de trouver une strat√©gie pour chaque client qui optimise soit leur chiffre d'affaires attendu, soit leur profit.

Pour aller plus loin, nous devrons essayer de :

- Modifier le niveau de remise offert globalement
- Pr√©dire la r√©ponse des clients √† cette remise (c'est-√†-dire la probabilit√© de r√©siliation) en fonction de la mani√®re dont cette remise affecte leurs prix, le chiffre d'affaires et la marge.
- Faites attention √† ce que nous ayons appliqu√© la remise √† toutes les variables concern√©es. Pour faciliter cela, nous pourrions vouloir r√©entra√Æner notre mod√®le en utilisant un ensemble de variables plus simple o√π nous savons que nous pouvons int√©grer correctement la remise dans les pr√©dicteurs.
- Trouver le niveau de remise qui √©quilibre la r√©tention des clients par rapport au co√ªt des faux positifs.

En fait, cela pourrait √™tre transform√© en un probl√®me d'optimisation 2D :

- Objectif : maximiser le chiffre d'affaires net (c'est-√†-dire en incluant les avantages des vrais positifs et le co√ªt des faux positifs)
- Variables de d√©cision :
  - Niveau de remise offert, et
  - Fraction de personnes √† qui une remise est offerte

Une strat√©gie encore plus sophistiqu√©e consiste √† trouver le bon niveau de remise pour chaque client qui maximise
 leur chiffre d'affaires ou leur marge pr√©vue .""")


if selected_option == "Conclusion et Recommandations" :
    st.subheader(" Le taux de r√©siliation est en effet √©lev√© dans la division PME, soit 9,7 % sur 14 606 clients.")

    st.subheader("Le mod√®le pr√©dictif est capable de pr√©dire la r√©siliation, mais le principal facteur n'est pas"
                 "la sensibilit√© au prix du client. La consommation annuelle, la consommation pr√©vue et la marge nette"
                 " sont les trois principaux facteurs.")

    st.subheader("La strat√©gie de r√©duction de 20 % est efficace, mais seulement si elle est cibl√©e de mani√®re "
                 "appropri√©e. Offrir une r√©duction uniquement aux clients de grande valeur ayant une probabilit√© √©lev√©e "
                 "de r√©siliation.")


